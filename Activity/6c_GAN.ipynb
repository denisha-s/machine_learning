{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 470 Activities and Case Studies\n",
    "\n",
    "1. For all activities, you are allowed to collaborate with a partner. \n",
    "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
    "\n",
    "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some considerations with regard to how these notebooks will be graded:\n",
    "\n",
    "1. Cells in which \"# YOUR CODE HERE\" is found are the cells where your graded code should be written.\n",
    "2. In order to test out or debug your code you may also create notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\". We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**. \n",
    "2. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
    "3. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will not receive points for your work in that section.\n",
    "4. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
    "5. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the \"assert\" statements will revert to the original. Make sure you don't edit the assert statements.\n",
    "6. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
    "7. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
    "8. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad4ef314459f67a08d5d2fb258135a4e",
     "grade": false,
     "grade_id": "cell-f1170f1f4c710ffc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Deep Learning - Generative Adversarial Networks\n",
    "\n",
    "In this exercise we will build a GAN using the MNIST dataset. If everything goes well and you're able to train the GAN correctly, you should be able to generate handwritten digits that never existed before, though it may require a lot of hyperparameter experimentation and lengthy training times to get high-quality images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ddf39657aec4adb6",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Reshape, Input\n",
    "from tensorflow.keras import Model, Sequential\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.get_logger().setLevel('ERROR')  # Don't display tensorflow warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "img_shape = list(x_train[0].shape)\n",
    "print(img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e23d5c4afedc7d8",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Set some hyperparameters\n",
    "\n",
    "latent_dim = 100  # This is the dimension of the random noise we'll use for the generator\n",
    "batch_size = 128\n",
    "n_train_steps = 1000  # We'll specify number of training steps (batches) rather than number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Generator model\n",
    "\n",
    "The generator takes in a latent vector and outputs a 2D image. Below, you'll build a model with three hidden Dense layers and a Dense output layer. The final output is then Reshape'd to the image size.\n",
    "\n",
    "We haven't discussed [BatchNorm layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) much, if at all, but we'll include a BatchNorm layer after each hidden Dense layer. BatchNorm layers do a batch-by-batch normalization of the data, which can greatly accelerate training convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6eee199330a2b5d78908f4cc14a49193",
     "grade": false,
     "grade_id": "cell-0f10b162ea8716db",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of layers that specify the architecture of the generator, \n",
    "# and name that list \"generator_layers\".\n",
    "#\n",
    "# Think about what the input to the generator is and what the output should be...\n",
    "#\n",
    "# Your model should take in a latent vector, process it with three hidden Dense layers\n",
    "# with number of nodes and activation of your chosing (try starting with 100 nodes\n",
    "# per layer and ReLU activation). After each hidden layer, add a BatchNormalization layer.\n",
    "# A final (additional) output Dense layer should have activation function that\n",
    "# maintains values between -1 and 1 (the range of \"pixel\" values).\n",
    "# Finalize the list of layers with a Reshape layer to get it to the correct image size.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "generator = Sequential(generator_layers)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82fe438946dff732bb7ce12d2829af99",
     "grade": true,
     "grade_id": "cell-a313f9e60d0791ba",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert generator\n",
    "assert len(generator_layers) == 8\n",
    "assert isinstance(generator_layers[0], Dense)\n",
    "assert isinstance(generator_layers[2], Dense)\n",
    "assert isinstance(generator_layers[4], Dense)\n",
    "assert isinstance(generator_layers[6], Dense)\n",
    "assert isinstance(generator_layers[1], BatchNormalization)\n",
    "assert isinstance(generator_layers[3], BatchNormalization)\n",
    "assert isinstance(generator_layers[5], BatchNormalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4427834fc0b2a565714d4da6eaaa6ef8",
     "grade": false,
     "grade_id": "cell-e56192cee898c737",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of layers for the discriminator model and name it \"discriminator_layers\"\n",
    "#\n",
    "# Think about what the input and output for a discriminator model are...\n",
    "#\n",
    "# The discriminator model should have two Dense layers.\n",
    "# You can chose the number of neurons in each layer. 512 and 256 for the first\n",
    "# and second layers, respectively, are reasonable starting points.\n",
    "# Add the appropriate output layer and activation function.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "discriminator = Sequential(discriminator_layers)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31b52ab8ee3d16141955d2bb11f34964",
     "grade": true,
     "grade_id": "cell-c99af873a3c82d84",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert discriminator\n",
    "assert len(discriminator_layers) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42af5aee1b1083b99bd39a8e9a5bf09e",
     "grade": false,
     "grade_id": "cell-1017fde57e42b55a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assemble the entire GAN\n",
    "\n",
    "Training GANs is a bit more complex than other networks so the below code is provided for you as is. You may reuse this code for your own edification but are not expected to have figured it out on your own.\n",
    "\n",
    "You are also likely to not be able to get good results in the time allocated for this exercise. Although you might if you try a variety of hyperparameters and let your models train for quite some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4bedce34f78b4e49",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "discriminator.compile(loss='binary_crossentropy',optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# The generator takes noise as input and generates images\n",
    "z = Input(shape=(latent_dim,))\n",
    "img_fake = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated images as input and determines validity (\"realness\")\n",
    "realness = discriminator(img_fake)\n",
    "\n",
    "combined = Model(z, realness)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6dc202d2ea46f43c",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Rescale image data to [-1, 1]\n",
    "x_train = x_train / 127.5 - 1.\n",
    "\n",
    "# Ground truth labels\n",
    "labels_real = np.ones((batch_size, 1))\n",
    "labels_fake = np.zeros((batch_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The code below trains the GAN by alternating between updates to the discriminator and updates to the generator. In the plots that are created during ongoing training, observe the generated images and the losses of both models. At some points during training you might see example of mode collapse--as indicated by many or all the generated images looking like the same (noisy) number (or shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a2640aa416d92a4",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "loss_history_disc = []\n",
    "loss_history_gen = []\n",
    "\n",
    "rows, cols = 5, 5  # Number of subplot rows and columns\n",
    "\n",
    "t_start = time.time()\n",
    "for step in range(n_train_steps):\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "\n",
    "    # Select a random batch of images\n",
    "    idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "    imgs = x_train[idx]\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "    # Generate a batch of new images\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Train the discriminator\n",
    "    discriminator.trainable = True\n",
    "    loss_disc_real, acc_disc_real = discriminator.train_on_batch(imgs, labels_real)\n",
    "    loss_disc_fake, acc_disc_fake = discriminator.train_on_batch(gen_imgs, labels_fake)\n",
    "    loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "    loss_history_disc.append(loss_disc)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "\n",
    "    # Train the generator (push the discriminator to predict fake images as real)\n",
    "    discriminator.trainable = False\n",
    "    loss_gen = combined.train_on_batch(noise, labels_real)\n",
    "    loss_history_gen.append(loss_gen)\n",
    "    \n",
    "    if step%50==0 or step==n_train_steps-1:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (rows*cols, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        # Rescale images to [0, 1] for plotting\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.clf()\n",
    "        cnt = 0\n",
    "        for i_row in range(rows):\n",
    "            for j_col in range(cols):\n",
    "                plt.subplot(rows, cols, cnt+1)\n",
    "                plt.imshow(gen_imgs[cnt], cmap='gray')\n",
    "                plt.axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"Step: {step+1}\")\n",
    "        \n",
    "        plt.figure(2)\n",
    "        plt.clf()\n",
    "        plt.plot(loss_history_disc, label='disc')\n",
    "        plt.plot(loss_history_gen, label='gen')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        print(f'Trained for {step+1} steps in {time.time()-t_start:0.1f} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses\n",
    "\n",
    "Note that the losses of the generator and discriminator oscillate, and do not generally decrease across training, unlike training losses we've observed in the past. This is because the generator and discriminator are battling against one another, never allowing the other to get the upper hand. Nonetheless, both models are learning, as we can see by the subjective improvement of the fake images over the training period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-657e15b84df17cb8",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate and display a few more fake digits\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "noise = np.random.normal(0, 1, (4, latent_dim))\n",
    "gen_img = generator.predict(noise)\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(gen_img[i], cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52b6d1a7fc96d9f64a1b4cf38e6737a2",
     "grade": false,
     "grade_id": "cell-9f46f0d2316e9dc9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
     "grade": false,
     "grade_id": "feedback",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def feedback():\n",
    "    \"\"\"Provide feedback on the contents of this exercise\n",
    "    \n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
     "grade": true,
     "grade_id": "feedback-tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
