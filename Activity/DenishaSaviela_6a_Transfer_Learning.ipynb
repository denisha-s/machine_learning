{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzS2hdTyB7nM"
      },
      "source": [
        "## CSCI 470 Activities and Case Studies\n",
        "\n",
        "1. For all activities, you are allowed to collaborate with a partner.\n",
        "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
        "\n",
        "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlOKpjaIB7nQ"
      },
      "source": [
        "Some considerations with regard to how these notebooks will be graded:\n",
        "\n",
        "1. Cells in which \"# YOUR CODE HERE\" is found are the cells where your graded code should be written.\n",
        "2. In order to test out or debug your code you may also create notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\". We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**.\n",
        "2. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
        "3. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will not receive points for your work in that section.\n",
        "4. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
        "5. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the \"assert\" statements will revert to the original. Make sure you don't edit the assert statements.\n",
        "6. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
        "7. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
        "8. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3a3b267c03d969d8512c4547ca52880f",
          "grade": false,
          "grade_id": "cell-c9d79ad28495f780",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "Mzuvs6XOB7nS"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "We've already looked at one example of transfer learning. For NLP topic classification, we used various feature representations including using GloVe representations as the chosen word embedding. That was one example of transfer learning where the approach we used was a feature representation transfer.\n",
        "\n",
        "In this exercise we will apply transfer learning to computer vision where we will incorporate both parameter transfer as well as feature representation transfer. You can take a look at the available pre-trained models in keras on the [applications page](https://keras.io/applications) in the documentation. In modern ML practice, we often use transfer learning to gain the benefit of state of the art models, rather than build a model from scratch.\n",
        "\n",
        "For thie exercise, we will use the [CIFAR10 dataset](https://keras.io/datasets/#cifar10-small-image-classification)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-efba043cb3f09871",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "id": "Bt5nLvyZB7nT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTEtuO5dB7nV"
      },
      "source": [
        "## Load, examine, and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H77x4q3AB7nW",
        "outputId": "5549fb6a-9fd1-4a57-894a-860f1d0430ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "num_classes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt4IvscMB7nW",
        "outputId": "875e0c65-f84a-4e10-9cd1-daadb1577af1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 14644370800337024901\n",
              " xla_global_id: -1]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# This code simply lets us know if the TensorFlow has access\n",
        "# to a GPU, or just a CPU\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-da2aa308afa6cd45",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEoVFNhEB7nX",
        "outputId": "b8ac5bb4-13d3-484a-bbf9-ad734d1dd006"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (50000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Let's see how many training samples we have, and the size of the images\n",
        "\n",
        "x_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6luNgXFEB7nX",
        "outputId": "d92b83d1-8570-4981-f9ec-026b42264916"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# CIFAR-10 consists of 10 object classes. Let's confirm this.\n",
        "\n",
        "np.unique(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-00565449f93db419",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwCR5PPQB7nY",
        "outputId": "cd3b0971-6825-4a14-ea67-dc976318450a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 32, 32, 3), (10000, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# See how many test samples we have, and confirm that array sizes meet our expectations\n",
        "\n",
        "x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdTSCfdmB7nY",
        "outputId": "65d9fa7f-58c3-458a-eb41-b1ada0af7ffa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 10), (10000, 10), 0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# We need to convert the integer-based class labels to categorical, one-hot\n",
        "# encoding vectors. Note that this is analogous to our use of get_dummies()\n",
        "# in pandas, which serves the same purpose except for DataFrames rather than\n",
        "# numpy arrays.\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "y_train.shape, y_test.shape, y_train.min(), y_train.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "50f583dad12a72fea9f8dce818d8391f",
          "grade": false,
          "grade_id": "cell-46a3e8197a25e135",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "atgtKKCJB7nZ"
      },
      "source": [
        "## Extract parameters from a pre-trained model\n",
        "\n",
        "We'll use a VGG-16 model which has been trained on the [ImageNet ILSVRC](https://www.image-net.org/challenges/LSVRC/) image corpus of 1000 image categories. VGG-16 is a 16-layered CNN model. We'll keep the early convolutional layers and use them as a front-end for our new model, leveraging those convolutional layers for feature extraction. We'll discard the subsequent fully-connected layers (the \"top\") of the VGG-16 model, and instead, build a new \"top\" and train it for classification of CIFAR-10 images.\n",
        "\n",
        "Note that this process is taking the feature extraction knowledge learned from the ImageNet data and \"transferring\" it to our new model.\n",
        "\n",
        "You'll need to reference the [VGG16 Keras documentation](https://keras.io/api/applications/vgg/#vgg16-function) to accomplish this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "db0fd117651360de3fe6e8d5d4de26ec",
          "grade": false,
          "grade_id": "cell-cbf2a8f1fde34aa8",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3LGoDpLB7nZ",
        "outputId": "e4812595-8bef-4d8d-d8e6-3177652e7480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load a VGG16 model with ImageNet weights, without the fully-connected (\"top\") layers.\n",
        "#\n",
        "# Note that convolutional layers are independent of actual height and width image\n",
        "# dimesions, so you have to use \"input_shape\" to inform the VGG16 object/model what\n",
        "# the those dimensions will be.\n",
        "#\n",
        "# Save your model as \"vgg_model\".\n",
        "\n",
        "# YOUR CODE HERE\n",
        "vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(32,32,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c0ef0c7b96d2bce168920de695e896d4",
          "grade": true,
          "grade_id": "cell-7cc41b089035d39a",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "Id3r-ICfB7na"
      },
      "outputs": [],
      "source": [
        "assert vgg_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4sxVuBaB7na",
        "outputId": "897a2d09-c75d-4464-fb7c-260eb5c5ace8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 4, 4, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vgg_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhf-LHT0B7nb",
        "outputId": "6f8b73af-b89a-401d-8e73-c10e36a81777"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Let's look at an image sample, and then you'll confirm that you\n",
        "# can use the pre-trained model for feature extraction.\n",
        "\n",
        "example = x_train[4]\n",
        "example.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "JWx1Kft_B7nb",
        "outputId": "ef3b7417-7b73-4a13-8777-530d976a9646"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(example)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c48c0df88a2511c20f597fde3c82f8ae",
          "grade": false,
          "grade_id": "cell-9efc493f08237ae8",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0XbWOCgB7nb",
        "outputId": "595bd664-e399-412a-e673-52103818604b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 484ms/step\n"
          ]
        }
      ],
      "source": [
        "# Use vgg_model to extract the features for the example image.\n",
        "# Save the output as \"example_vgg\".\n",
        "\n",
        "# YOUR CODE HERE\n",
        "example_vgg = vgg_model.predict(np.expand_dims(example, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5a354bd81201b8539fa818c87b10bd40",
          "grade": true,
          "grade_id": "cell-0c7351fe695136f1",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "iVRyIukGB7nc"
      },
      "outputs": [],
      "source": [
        "assert np.squeeze(example_vgg).shape == (512,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5af710269a5562573d0839264f3b0832",
          "grade": false,
          "grade_id": "cell-8d58e4bfcd6581ec",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "whr_ewbDB7nc"
      },
      "source": [
        "##  Build and train a CIFAR-10 classifier model\n",
        "\n",
        "Now let's create our own fully-connected layers and append those to the top of the `vgg_model`. We can then train our model on the CIFAR10 dataset and predict its classes. In the following code, you'll want to use Keras' [functional API](https://www.tensorflow.org/guide/keras/functional) that we've used before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "03dbffda429732ed16f9b7b1b77cd4c2",
          "grade": false,
          "grade_id": "cell-2dbbcd51c1a9bcf5",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "7_xZ3e3oB7nc"
      },
      "outputs": [],
      "source": [
        "# Use the Keras function API to create a new set of layers that\n",
        "# will take input from the output of the VGG convolutional layers.\n",
        "#\n",
        "# - Flatten the output of the VGG convolutional layers\n",
        "# - Add 3 Dense layers with 256 units each and an appropriate activation function.\n",
        "# - Add a final output layer of proper size and activation function.\n",
        "#\n",
        "# Save the output of the final layer as \"predictions\".\n",
        "\n",
        "# Below, \"x\" is the output of the VGG16 layers. It will serve as the\n",
        "# input to the new layers you'll create below.\n",
        "x = vgg_model.output\n",
        "\n",
        "# YOUR CODE HERE\n",
        "x = Flatten(data_format=None)(x)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "predictions = Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "# Now we can create a single Model that takes data from the input layer\n",
        "# of the VGG convolutional layers to the output of your fully-connected layers.\n",
        "vgg_complete = Model(vgg_model.input, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "97cf91aa294f3ffbbab690eebec68821",
          "grade": true,
          "grade_id": "cell-ccdc0af6567cf083",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "wXqAqZRYB7nd"
      },
      "outputs": [],
      "source": [
        "assert len(vgg_complete.layers) - len(vgg_model.layers) == 5\n",
        "assert vgg_complete.layers[-2].units == 256\n",
        "assert vgg_complete.layers[-3].units == 256\n",
        "assert vgg_complete.layers[-4].units == 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJG2NkQ-B7nd",
        "outputId": "9c75989e-c913-448f-9ebd-11adc1404195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 4, 4, 512)         1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,980,170\n",
            "Trainable params: 14,980,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vgg_complete.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO9HzM3zB7nd"
      },
      "source": [
        "### Train\n",
        "\n",
        "Train your model using as many epochs as you'd like, observing it's convergence behavior.  \n",
        "__But you must set `epochs=1` and re-run your notebook before turning it in.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "iKHVIo_NB7ne"
      },
      "outputs": [],
      "source": [
        "# We want to train the parameters of your new fully-connected layers,\n",
        "# but not those of the convolutional layers. This is because we want to\n",
        "# preserve the feature extraction capabilities of those layers as\n",
        "# learned from the much larger ImageNet ILSVRC data set. \"Freezing\"\n",
        "# the convolutional parameters will also allow our model to train more quickly.\n",
        "#\n",
        "# Briefly run this and the code cell below both with and without with \"trainable\" setting\n",
        "# set to False, while observing the estimated epoch training duration (\"ETA\").\n",
        "# Aterwards, but sure to set it back to False, and restart/rerun your notebook.\n",
        "for layer in vgg_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-555ef11edc47dba7",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8g4VS3jB7ne",
        "outputId": "3c5e547e-6750-4dfb-a390-199318c7696c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "391/391 [==============================] - 615s 2s/step - loss: 1.6503 - accuracy: 0.4966\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7b5344ee8fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# This cell will take a while. Double check your work before continuing.\n",
        "#\n",
        "# If you set n_epochs greater than 1, to better observe convergence,\n",
        "# reset it to 1 before turning in our notebook.\n",
        "\n",
        "n_epochs = 1\n",
        "vgg_complete.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "vgg_complete.fit(x_train, y_train, epochs=n_epochs, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "66508aa829c910e32af725078e9b4718",
          "grade": false,
          "grade_id": "cell-ce8717f26f9f5d79",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z9rS2WbB7ne",
        "outputId": "d121f293-5d00-4862-87b4-ff9f03c818ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - 126s 2s/step - loss: 1.2783 - accuracy: 0.5607\n"
          ]
        }
      ],
      "source": [
        "# Using your model, get the accuracy score for the test set\n",
        "# and save it as \"acc\".\n",
        "\n",
        "# YOUR CODE HERE\n",
        "loss,acc = vgg_complete.evaluate(x_test, y_test, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-7de667b2ebfee86b",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAllaeIHB7nf",
        "outputId": "40dafbcd-ed7d-451c-e921-ae3e5382f7dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random chance accuracy on this task is approximately 0.1.\n",
            "Your model's accuracy is 0.561.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Random chance accuracy on this task is approximately 0.1.\\nYour model's accuracy is {acc:.3f}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "18b16fcf9ae16b9cd14912d902f038db",
          "grade": true,
          "grade_id": "cell-eb960da5a7f7d0ab",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "LmhzeNCXB7nf"
      },
      "outputs": [],
      "source": [
        "assert acc > 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b03df30093f3dd39ef4d52fc85c91fd4",
          "grade": true,
          "grade_id": "cell-b6bf06c40977fcab",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "NQ1gyPEoB7nf"
      },
      "outputs": [],
      "source": [
        "assert n_epochs==1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV0IZfGTB7nf"
      },
      "source": [
        "## Additional Exercises [optional]\n",
        "\n",
        "For further edification, consider comparing results from up to four models/situations. The ones using VGG16 will take longer to compute, but still only 10-15 minutes each if you train only for ~5 epochs. Consider training them for 10 epochs each, for clear differentiation of results.\n",
        "\n",
        "Be sure to look at the final accuracy score of the __test set__, not just those of the training set.\n",
        "\n",
        "1. Use the VGG-16 convolutional layers, but train all model parameters __from scratch__. To do so, simply set `weights=None` when you create `vgg_model` using `VGG16`.\n",
        "2. [You did this one already] Use pre-trained VGG-16 convolutional layers, and keep those layers \"frozen\" during training on CIFAR-10 data.\n",
        "3. Use pre-trained VGG-16 convolutional layers but in contrast to #2 above, do not freeze those layers during training on CIFAR-10 data (set `layer.trainable=True` a few cells above). This is called __fine-tuning__.\n",
        "4. VGG-16 is a large model. How well does a __smaller CNN model__ do, when trained from scratch? We did that for the MNIST model in the CNN Activity. Copy that model here, adjust the input shape to (32, 32, 3) rather than (28, 28, 1), then train and evalutate that model.\n",
        "\n",
        "__Which of the above approaches gives the best test set performance (after sufficient training epochs)?__\n",
        "\n",
        "\n",
        "__Be sure to restore all original settings before your final notebook run and submittal__:\n",
        "- `weights = 'imagenet'`\n",
        "- `layer.trainable = False`\n",
        "- `n_epochs = 1`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "feac2b34a20fc9f52ac6606cac738ac9",
          "grade": false,
          "grade_id": "cell-d77ae0b5a37d13a7",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "nbGI7PbIB7ng"
      },
      "source": [
        "## Feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
          "grade": false,
          "grade_id": "feedback",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "3XGMrwQ_B7ng"
      },
      "outputs": [],
      "source": [
        "def feedback():\n",
        "    \"\"\"Provide feedback on the contents of this exercise\n",
        "\n",
        "    Returns:\n",
        "        string\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    return \"No comment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
          "grade": true,
          "grade_id": "feedback-tests",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CGg15yAkB7ng",
        "outputId": "15280e18-56ec-4f2e-8b28-a5a59b3f657b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'No comment'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "feedback()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}