{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 470 Activities and Case Studies\n",
    "\n",
    "1. For all activities, you are allowed to collaborate with a partner. \n",
    "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
    "\n",
    "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some considerations with regard to how these notebooks will be graded:\n",
    "\n",
    "1. Cells in which \"# YOUR CODE HERE\" is found are the cells where your graded code should be written.\n",
    "2. In order to test out or debug your code you may also create notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\". We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**. \n",
    "2. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
    "3. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will not receive points for your work in that section.\n",
    "4. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
    "5. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the \"assert\" statements will revert to the original. Make sure you don't edit the assert statements.\n",
    "6. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
    "7. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
    "8. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c72cb50c0e638cb004dec850c91c6f1b",
     "grade": false,
     "grade_id": "cell-0ee2026dc35233b5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c42419557b8f132",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[toy_text]\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from google.colab.patches import cv2_imshow\n",
    "from google.colab import output\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aed4b7721924092d453e0f9a3fe7755a",
     "grade": false,
     "grade_id": "cell-36cbf46419269559",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## The Environment\n",
    "\n",
    "We will be using [OpenAI's gym](https://gym.openai.com/docs/) for rendering environments and we will specifically use the [Taxi-v3](https://gym.openai.com/envs/Taxi-v3/) environment for this exercise. In the next few cells we'll visualize the environment and explain the rules and rewards. It will be up to you to create an agent that interacts with the environment and learns a policy (Q table) from those interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae8bb9fd4f2545d3",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the Taxi-v3 environment\n",
    "env = gym.make(\"Taxi-v3\",render_mode=\"rgb_array\").env\n",
    "\n",
    "# Set the RNG so we'll get the same results with every notebook run\n",
    "# env.seed(0)\n",
    "env.reset()\n",
    "\n",
    "# Visualize the initial (and current) state\n",
    "print(f\"Current State: {env.s}\")\n",
    "\n",
    "cv2_imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be6ee80cdc843fe99e8a2c9b705ad709",
     "grade": false,
     "grade_id": "cell-373c22c099142701",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The above section just rendered an example view of the environment. For the Taxi-v3 environment,\n",
    "\n",
    "1. The location of the taxi is randomly chosen with each new play of the environment.\n",
    "1. Red, Green, Purple, Blue areas are all the possible pickup or dropoff locations for a passenger. These locations are the same for every play of the environment.\n",
    "1. The passenger appears at the pickup location, which is one of the 4 available areas. The pickup location is randomly chosen with each new play of the environment.\n",
    "1. The building appears at the passenger's dropoff location. The dropoff location is randomly chosen with each new play of the environment.\n",
    "\n",
    "The reward scheme for this environment is as follows, \"your job is to pick up the passenger at one location and drop them off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73b4814ef8176fe2",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# The gym environment contains info on how many states are possible and\n",
    "# how many actions an agent can take. Let's find out...\n",
    "\n",
    "print(f\"The action space is discrete with {env.action_space.n} possibilities.\")\n",
    "print(f\"The observation (state) space is discrete with {env.observation_space.n} possibilities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f1e871a27603b57cbe8d764492e3360",
     "grade": false,
     "grade_id": "cell-eadba99486bd8679",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The following actions are possible in the environment:\n",
    "\n",
    "1. Move south\n",
    "1. Move north\n",
    "1. Move east\n",
    "1. Move west\n",
    "1. Pick up passenger\n",
    "1. Drop off passenger\n",
    "\n",
    "Note that there are 500 state space possibilties. Think about the different states that may exist, and how that totals to 500 possibilities. See the [blog post](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/) for a full explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Q table\n",
    "\n",
    "In the cell below, implement a function that initializes a Q table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b756e8cc77a72df6c71551270e34ccc",
     "grade": false,
     "grade_id": "cell-5f22453e53fcf9cf",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize_q_table(env):\n",
    "    \"\"\"Initialize a Q table for an environment with all 0s\n",
    "    \n",
    "    Args:\n",
    "        env (gym.envs): The environment\n",
    "    \n",
    "    Returns:\n",
    "        np.array: The Q table of shape (observation space size, action space size)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8da15df3275ef5f40316b10aa6584f65",
     "grade": true,
     "grade_id": "cell-1f3967db32df3828",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert initialize_q_table(env).shape == (500, 6)\n",
    "xenv = gym.make(\"FrozenLake-v1\").env\n",
    "assert initialize_q_table(xenv).shape == (16, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action selection\n",
    "\n",
    "In the cell below, implement a function that chooses an action, given a particular environment state (row of a Q table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2954a57d58b91250a3be6e256579eb1e",
     "grade": false,
     "grade_id": "cell-4b276160d41b4aa0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def select_action(q_row, method, epsilon=0.5):\n",
    "    \"\"\"Select the appropriate action given a Q table row for the state and a chosen method\n",
    "    \n",
    "    Args:\n",
    "        q_row (np.array): The row from the Q table to utilize\n",
    "        method (str): The method to use, either \"random\" or \"epsilon\"\n",
    "        epsilon (float, optional): Defaults to 0.5. The epsilon value to use for\n",
    "                                   epislon-greedy action selection\n",
    "    \n",
    "    Raises:\n",
    "        NameError: If method specified is not supported\n",
    "    \n",
    "    Returns:\n",
    "        int: The index of the action to apply\n",
    "    \"\"\"\n",
    "    if method not in [\"random\", \"epsilon\"]:\n",
    "        raise NameError(\"Undefined method.\")\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec78e64fb3d0b6d16e2eaf9019a600cb",
     "grade": true,
     "grade_id": "cell-436a8b9b98845dd8",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert select_action(np.array([1,2,3,4]), \"epsilon\", epsilon=0) == 3\n",
    "assert select_action(np.array([1,2,3,4]), \"epsilon\", epsilon=1) in range(4)\n",
    "assert select_action(np.array([1,2,3,4]), \"random\") in range(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7094122cd640e38565c150462077a45",
     "grade": false,
     "grade_id": "cell-2171ed4400886241",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Actions and updates\n",
    "\n",
    "The `env.step(action)` method takes a parameter that is the (index of the) action the agent decides to apply and returns 4 values:\n",
    "1. The new state\n",
    "1. The received reward\n",
    "1. Whether or not the task ask been completed\n",
    "1. Miscellaneous information\n",
    "\n",
    "Let's look at an example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-219e07fb8178585a",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "action = 0\n",
    "vals = env.step(action)\n",
    "print(f\"An example returned from a step with action 0:\")\n",
    "print(vals)\n",
    "print('')\n",
    "print(f\"New state: {vals[0]}\")\n",
    "print(f\"Return (reward): {vals[1]}, based on performing action {action}\")\n",
    "print(f\"Task complete: {vals[2]}\")\n",
    "print(f\"Additional info: {vals[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "During Q-learning, we update our agent's Q-table after each action, based on the previous state, the action, and the reward. In the cell below implement the update equation discussed in class. Note that the `calculate_new_q_val` function only returns the updated Q value for that specific (state, action) pair. It does not return the entire table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f930657e2f5fb7602cdb97d7ef001d78",
     "grade": false,
     "grade_id": "cell-0234e0a3e6f2eb0a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_new_q_val(q_table, state, action, reward, next_state, alpha, gamma):\n",
    "    \"\"\"Calculate the updated Q table value for a particular state and action given the necessary parameters\n",
    "    \n",
    "    Args:\n",
    "        q_table (np.array): The Q table\n",
    "        state (int): The current state of the simulation's index in the Q table\n",
    "        action (int): The current action's index in the Q table\n",
    "        reward (float): The returned reward value from the environment\n",
    "        next_state (int): The next state of the simulation's index in the Q table (Based on the environment)\n",
    "        alpha (float): The learning rate\n",
    "        gamma (float): The discount rate\n",
    "    \n",
    "    Returns:\n",
    "        float: The updated action-value expectation for the state and action\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff0ca3da6b4ede2d0cd6899c956b1136",
     "grade": true,
     "grade_id": "cell-f5c631dceb6c6cf0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_q = np.array([[1,2,3,4], [1,2,3,4], [1,2,3,4]])\n",
    "assert -0.05 < calculate_new_q_val(test_q, 0, 1, 10, 1, 0.1, 0.2) - 2.88 < 0.05\n",
    "assert -0.05 < calculate_new_q_val(test_q, 0, 1, 1, 1, 0.1, 0.1) - 1.94 < 0.05\n",
    "assert -0.05 < calculate_new_q_val(test_q, 0, 1, -11, 2, 0.1, 0.1) - 0.74 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "You'll train two agents using an epsilon greedy approach with two different epsilon values. Recall that a higher epsilon value means that the agent will do more exploration and less exploitation, during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b7b2eccdd9ee9f58",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "params1 = {\n",
    "    \"method\": \"epsilon\",\n",
    "    \"epsilon\": 0.1,\n",
    "    \"alpha\": 0.1,\n",
    "    \"gamma\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = {\n",
    "    \"method\": \"epsilon\",\n",
    "    \"epsilon\": 0.3,\n",
    "    \"alpha\": 0.1,\n",
    "    \"gamma\": 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `train_agent` function below, an agent is trained over the course of many simulations of the environment/game. Each simulation is run until the task is completed (which could take longer during early learning, when the agent has a poor Q table--e.g., doesn't \"know anything\").\n",
    "\n",
    "You need to create code that will (1) select an action based on the current state, (2) execute that action in the gym environment, and (3) update the Q table based on the results returned by the gym environment.\n",
    "\n",
    "To aid in visualization of how well and quickly an agent is learning you will (4) add the step reward to the total reward, for the given simulation iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb452a3323108e685489b286a424120e",
     "grade": false,
     "grade_id": "cell-13af0762ec7af495",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_agent(env, params, n_sims=100):\n",
    "    \"\"\"Run a training simulation in an environment and return its Q table\n",
    "    \n",
    "    Args:\n",
    "        env (gym.envs): The environment to train in\n",
    "        params (dict): The parameters needed to train the simulation, stored with these keys:\n",
    "                         'method' (for action selection--either 'epsilon' or 'random'),\n",
    "                         'epsilon',\n",
    "                         'alpha',\n",
    "                         'gamma'\n",
    "        n_sims (int, optional): Defaults to 100. The number of simulations to run for training\n",
    "    \n",
    "    Returns:\n",
    "        q_table (np.array): The trained Q table from the simulation\n",
    "        total_rewards (np.array): Total (summed) rewards for each of the n_sim training simulations\n",
    "    \"\"\"\n",
    "    q_table = initialize_q_table(env)\n",
    "    total_rewards = np.zeros(n_sims)\n",
    "    \n",
    "    for i_sim in range(n_sims):\n",
    "        env.reset()\n",
    "        current_state = env.s\n",
    "        done = False        \n",
    "        \n",
    "        while not done:\n",
    "            # 1. Get the next action based on current state\n",
    "            # 2. Execute that action and get the results from the environment\n",
    "            # 3. Update the Q table\n",
    "            # 4. Add the step reward to the reward total, for this (i_sim'th) simulation\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # Prep for next iteration\n",
    "            current_state = next_state \n",
    "\n",
    "        if (i_sim+1) % 1000 == 0:\n",
    "            print(f\"Simulation #{i_sim+1:,} complete.\")\n",
    "        \n",
    "    return q_table, total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train our two agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9edcffba53e06847",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_train_simulations = 10000\n",
    "q_table1, total_rewards = train_agent(env, params1, n_train_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a rough visual of how quickly that agent learned...\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "idx = np.arange(0, n_train_simulations, 10)\n",
    "plt.plot(idx, total_rewards[idx], '.')\n",
    "plt.xlabel('Training simulation')\n",
    "plt.ylabel('Total reward')\n",
    "plt.grid()\n",
    "_ = plt.axis([0, n_train_simulations, -500, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_train_simulations = 10000\n",
    "q_table2, total_rewards = train_agent(env, params2, n_train_simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a rough visual of how quickly that agent learned...\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "idx = np.arange(0, n_train_simulations, 10)\n",
    "plt.plot(idx, total_rewards[idx], '.')\n",
    "plt.xlabel('Training simulation')\n",
    "plt.ylabel('Total reward')\n",
    "plt.grid()\n",
    "_ = plt.axis([0, n_train_simulations, -500, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Having trained two agents, new we want to test them. The agents are no longer learning, so we'll want to execute the actions that have the highest action-value for a given state--that is, there is no randomness to the agents' actions.\n",
    "\n",
    "In the cell below your code should (1) determine the best action for the current state and (2) execute that action in the gym environment.\n",
    "\n",
    "Note that because the agent is not acting deterministically, it may get \"stuck\" and never complete the task, so there is code that limits the duration (steps) of the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6647971977b41fccefcf9570ba78c0e6",
     "grade": false,
     "grade_id": "cell-2100a877594cd931",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def test_agent(env, q_table, n=100, render=False):\n",
    "    \"\"\"Test an agent using a pre-trained Q table\n",
    "    \n",
    "    Args:\n",
    "        env (gym.envs): The environment to test\n",
    "        q_table (np.array): The pretrained Q table\n",
    "        n (int, optional): Defaults to 100. The number of test iterations to run\n",
    "        render (bool, optional): Defaults to False. Whether to display a rendering of the environment\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Array of length n with each value being the cumulative reward achieved in the simulation\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        env.reset()\n",
    "        current_state = env.s\n",
    "\n",
    "        tot_reward = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            # 1. Determine the best action\n",
    "            # 2. Execute that action\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            tot_reward += reward\n",
    "            step +=1\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Simulation: {i + 1}\")\n",
    "                cv2_imshow(env.render())\n",
    "                print(f\"Step: {step}\")\n",
    "                print(f\"Current State: {current_state}\")\n",
    "                print(f\"Action: {action}\")\n",
    "                print(f\"Reward: {reward}\")\n",
    "                print(f\"Total rewards: {tot_reward}\")\n",
    "                sleep(.2)\n",
    "            if step == 50:\n",
    "                print(\"Agent got stuck. Quitting...\")\n",
    "                sleep(.5)\n",
    "                break\n",
    "        \n",
    "        rewards.append(tot_reward)\n",
    "    \n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each run of the simulation environment has a different initial state (passenger location and destination, and taxi location) we'll test the agents multiple times, and the compare median scores (total reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-18ab739306cf86ff",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# To see the testing simulations running, set:\n",
    "#    render=True\n",
    "# You should do this for a least a few simulations, to\n",
    "# qualitatively observe the agent. You can then stop\n",
    "# the notebook kernel and reset render=False.\n",
    "\n",
    "rewards1 = test_agent(env, q_table1, 100, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b4310173795ca573",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# To see the testing simulations running, set:\n",
    "#    render=True\n",
    "\n",
    "rewards2 = test_agent(env, q_table2, 100, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0a858504adc25ae1",
     "locked": false,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total reward for agent with params1: mean={np.mean(rewards1):0.2f}, median={np.median(rewards1)}\")\n",
    "print(f\"Total reward for agent with params2: mean={np.mean(rewards2):0.2f}, median={np.median(rewards2)}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "n_bins = 20\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(rewards1, bins=n_bins)\n",
    "plt.xlabel('Total reward')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Agent with params1')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(rewards2, bins=n_bins)\n",
    "plt.xlabel('Total reward')\n",
    "plt.ylabel('Count')\n",
    "_ = plt.title('Agent with params2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "011702af6f1c5695a85a26f2c687948f",
     "grade": true,
     "grade_id": "cell-a4db87228ab068a8",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Your models may sometimes not pass the below asserts but you should be able to get it at least work sometimes\n",
    "# To avoid any issues with grading, we've commented them out.\n",
    "# To make the most out of this activity, please uncomment them and get them to at least occasionally pass\n",
    "# assert np.median(rewards1) > 5\n",
    "# assert np.median(rewards2) > 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your exploration\n",
    "\n",
    "Like an agent that is learning, you should explore how agent performance changes when you alter some of the learning parameters. You can re-run this notebook multiple times, altering the values in the `params` dicts.\n",
    "\n",
    "- How well and quickly does training go when using various values for epsilon, alpha, and gamma?\n",
    "- How well and quickly does training go when using the 'random' method (which is equivalent to epsilon==1)?\n",
    "- Does an agent improve if you train it for more than 10,000 simulation runs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0396d41ebf05afc94934500ec6d00c6",
     "grade": false,
     "grade_id": "cell-b19c1d376892e2c1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
     "grade": false,
     "grade_id": "feedback",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def feedback():\n",
    "    \"\"\"Provide feedback on the contents of this exercise\n",
    "    \n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
     "grade": true,
     "grade_id": "feedback-tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
