{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 470 Activities and Case Studies\n",
    "\n",
    "1. For all activities, you are allowed to collaborate with a partner. \n",
    "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
    "\n",
    "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some considerations with regard to how these notebooks will be graded:\n",
    "\n",
    "1. Cells in which \"# YOUR CODE HERE\" is found are the cells where your graded code should be written.\n",
    "2. In order to test out or debug your code you may also create notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\". We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**. \n",
    "2. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
    "3. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will not receive points for your work in that section.\n",
    "4. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
    "5. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the \"assert\" statements will revert to the original. Make sure you don't edit the assert statements.\n",
    "6. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
    "7. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
    "8. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretability\n",
    "\n",
    "In this exercise you'll use the [alibi](https://docs.seldon.io/projects/alibi/en/stable/) library to explain why some models make the predictions they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install the library\n",
    "\n",
    "# ! pip install alibi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and examine the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"DESCR\"])\n",
    "features = data[\"data\"]\n",
    "targets = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build two different classifiers and train them on the Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5aadf242d9b666592c1ac8e50fab88a1",
     "grade": false,
     "grade_id": "cell-1a782646da471586",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create 2 classifiers: a Random Forest model named \"rf_clf\", and a Linear SVM model named \"svm_clf\"\n",
    "#\n",
    "# Train them both on the training data.\n",
    "# Use them to predict the test data - saving predictions as \"y_rf_pred\" and \"y_svm_pred\".\n",
    "# You may want to use GridSearchCV to determine hyperparameters.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a450a84aaaaba72a74935ad60879ca31",
     "grade": true,
     "grade_id": "cell-0591079d074b5029",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(y_rf_pred) == 38\n",
    "assert isinstance(rf_clf, RandomForestClassifier) or isinstance(rf_clf, GridSearchCV)\n",
    "assert len(y_svm_pred) == 38\n",
    "assert isinstance(svm_clf, LinearSVC) or isinstance(svm_clf, GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Random forest test set accuracy: {accuracy_score(y_test, y_rf_pred):0.3f}.\")\n",
    "print(f\"SVM test set accuracy:           {accuracy_score(y_test, y_svm_pred):0.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get prediction explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we used a Linear SVM, we can easily determine the coefficients for the features:\n",
    "\n",
    "print(\"Each class gets a coefficient for each feature that helps us determine that feature's importance.\")\n",
    "\n",
    "print('\\nSVM feature coefficients:')\n",
    "if isinstance(svm_clf, LinearSVC):\n",
    "    print(svm_clf.coef_)\n",
    "elif isinstance(svm_clf, GridSearchCV):\n",
    "    print(svm_clf.best_estimator_.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how we can use explainers, namely the [AnchorTabular](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html#id3) explainer to understand why the models make the predictions they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi.explainers import AnchorTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alibi explainers follow a general structure of:\n",
    "\n",
    "1. Initialize the explainer, providing a prediction function, and explainer specific parameters. `exp = Explainer(predict_func, param_1, param_2, ...)`\n",
    "1. Fit the explainer to the training data (this step is explainer dependent) `exp.fit(train_data)`\n",
    "1. Explain a given sample `exp.explain(sample)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we reframe the prediction pipeline into a prediction function that we can use with the explainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf_func = lambda x: rf_clf.predict(x)\n",
    "svm_clf_func = lambda x: svm_clf.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instantiate the explainer using the prediction function and any parameters the explainer requires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_explainer = AnchorTabular(rf_clf_func, data[\"feature_names\"])\n",
    "rf_explainer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_explainer = AnchorTabular(svm_clf_func, data[\"feature_names\"])\n",
    "svm_explainer.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the explainer is set up, we can now use it to `.explain` samples! Pick a sample below to explain the two models' predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this value to choose a test sample\n",
    "index_to_explain = 7\n",
    "\n",
    "\n",
    "rf_explanation = rf_explainer.explain(X_test[index_to_explain])\n",
    "svm_explanation = svm_explainer.explain(X_test[index_to_explain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_explanation.anchor, rf_explanation.precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_explanation.anchor, svm_explanation.precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see what the model's explanation for the classification of that sample is. You can see that even with our relatively interpretable model of Linear SVMs, these explainers can provide a more direct and intuitive explanation for why a sample was labeled the way it was.\n",
    "\n",
    "Now that you've seen the general approach for these explainers, let's work on something a bit more complex. Now you'll have to create the models, the prediction function, and the explainers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset\n",
    "\n",
    "Explaining data from measured observations is simple enough. Now let's try explaining how images get labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train/255.\n",
    "x_test = x_test/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add noise to the MNIST images\n",
    "\n",
    "Because the nearly binary (black and white) nature of the MNIST images, the formation of superpixels is not representative of the type of behavior you would get from more complex, color images. To get closer to that type of behavior, we'll add noise to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the MNIST images\n",
    "\n",
    "x_train = x_train/2 + np.random.uniform(0, high=0.5, size=x_train.shape)\n",
    "x_train = np.clip(x_train, 0, 1)\n",
    "\n",
    "x_test = x_test/2 + np.random.uniform(0, high=0.5, size=x_test.shape)\n",
    "x_test = np.clip(x_test, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 12\n",
    "sample = x_train[sample_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.imshow(sample, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a CNN classifier and train it on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d3578cc1cd52cb6c14ddf6538fbbf75",
     "grade": false,
     "grade_id": "cell-0a1be97070f65961",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a neural network model that should do well on the MNIST dataset and save it to mnist_nn\n",
    "# Make the neural network sufficiently complex (at least 4 neural layers--max pooling and flatten\n",
    "# layers do not count). A CNN is recommended over a fully-connected feedforward network.\n",
    "\n",
    "# Save the neural network as \"mnist_nn\"\n",
    "#\n",
    "# *** Make sure you get at least 80% accuracy ***\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "mnist_nn.summary()\n",
    "\n",
    "mnist_nn.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "mnist_nn.fit(x_train.reshape(-1, 28, 28 ,1), y_train,\n",
    "             validation_data=(x_test.reshape(-1, 28, 28 ,1), y_test),\n",
    "             epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d6da45c071584c2fdd6ac4d464a8de5",
     "grade": true,
     "grade_id": "cell-e394a89e99cb2485",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(mnist_nn.layers) >= 4\n",
    "assert mnist_nn.evaluate(x_test.reshape(-1, 28, 28 ,1), y_test)[1] > 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get explanations for the MNIST model's predictions\n",
    "\n",
    "To work with images, we'll use the [AnchorImage](https://docs.seldon.io/projects/alibi/en/stable/methods/Anchors.html#id5) explainer. This explainer requires that we break up the image into \"superpixels\". We'll use the default 'slic' method to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi.explainers import AnchorImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aeb059d21787166e46af98024582da4b",
     "grade": false,
     "grade_id": "cell-2b6099964aa0a2be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create an explainer object using AnchorImage that explains the mnist_nn model you created.\n",
    "# Use the 'slic' method for as the segmentation function.\n",
    "# Save the explainer as \"mnist_explainer\".\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "492e1773d0063d2a1a2f8a2ed1f49592",
     "grade": true,
     "grade_id": "cell-f81be29970fdaf29",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(mnist_explainer, AnchorImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get an image, a prediction, and an explanation\n",
    "\n",
    "\n",
    "# Change this number to try out different samples\n",
    "image_index_to_explain = 6\n",
    "\n",
    "\n",
    "# Get the image\n",
    "image_to_explain = x_test[image_index_to_explain].reshape(28, 28, 1)\n",
    "\n",
    "# Get the prediction\n",
    "pred = mnist_nn.predict(image_to_explain.reshape(1, 28, 28, 1)).argmax()\n",
    "\n",
    "# Get the explanation\n",
    "mnist_image_explanation = mnist_explainer.explain(image_to_explain, threshold=0.95, p_sample=0.5)\n",
    "\n",
    "# Show results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image_to_explain[:,:,0], cmap=\"gray\")\n",
    "plt.title('Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mnist_image_explanation.segments)\n",
    "plt.title('Segments')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mnist_image_explanation.anchor[:,:,0], cmap=\"gray\")\n",
    "_ = plt.title(f'Anchor: Pixels that explain prediction of {pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of segmentation\n",
    "\n",
    "One thing you may have noticed is that the explanations are heavily dependent on the superpixels we identify. \n",
    "\n",
    "You may want to alter some of the parameters of the 'slic' segmentation function to see how it impacts the segmentation, and thus the explaining anchor. Add the keyword argument below to your `AnchorImage` instantiation above, and adjust the values.  \n",
    "`segmentation_kwargs={'n_segments': 15, 'compactness': 20, 'sigma': .5}`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ff0d8bed14ed80b45c8ae821e8967e6",
     "grade": false,
     "grade_id": "cell-523ebab8f9ab57b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def feedback():\n",
    "    \"\"\"Provide feedback on the contents of this exercise\n",
    "    \n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
